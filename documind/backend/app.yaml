# DocuMind Backend RAG Configuration
# Based on Pathway Question Answering RAG template

# Data sources for document indexing
$sources:
  # Local file system connector
  - !pw.io.fs.read
    path: /app/data
    format: binary
    with_metadata: true

# LLM Configuration
$llm: !pw.xpacks.llm.llms.OpenAIChat
  model: "gpt-4o-mini"
  retry_strategy: !pw.udfs.ExponentialBackoffRetryStrategy
    max_retries: 6
  cache_strategy: !pw.udfs.DefaultCache {}
  temperature: 0
  capacity: 8
  async_mode: "fully_async"

# Embedder Configuration
$embedder: !pw.xpacks.llm.embedders.OpenAIEmbedder
  model: "text-embedding-3-small"
  cache_strategy: !pw.udfs.DefaultCache {}
  retry_strategy: !pw.udfs.ExponentialBackoffRetryStrategy {}

# Text Splitter
$splitter: !pw.xpacks.llm.splitters.TokenCountSplitter
  max_tokens: 400

# Document Parser
$parser: !pw.xpacks.llm.parsers.DoclingParser
  table_parsing_strategy: "llm"
  async_mode: "fully_async"
  chunk: false
  cache_strategy: !pw.udfs.DefaultCache {}

# Vector Retriever Factory
$retriever_factory: !pw.indexing.UsearchKnnFactory
  reserved_space: 10000
  embedder: $embedder
  metric: !pw.indexing.USearchMetricKind.COS

# Document Store
$document_store: !pw.xpacks.llm.document_store.DocumentStore
  docs: $sources
  parser: $parser
  splitter: $splitter
  retriever_factory: $retriever_factory

# Question Answerer
question_answerer: !pw.xpacks.llm.question_answering.BaseRAGQuestionAnswerer
  llm: $llm
  indexer: $document_store
  search_topk: 6

# Server Configuration
host: "0.0.0.0"
port: 8080

# Persistence Configuration
persistence_mode: !pw.PersistenceMode.UDF_CACHING
persistence_backend: !pw.persistence.Backend.filesystem
  path: "/app/Cache"

# Error Handling
terminate_on_error: false
